{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "055a6b57-6754-45ea-a77c-a83899d50106",
   "metadata": {},
   "source": [
    "# mRAG-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c14a05-e9a9-4604-aa7d-13654a43a1b1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "07480d30-7bff-4e30-939d-d75f53758d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import requests\n",
    "import httpx\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d1368d-901b-4f3e-876b-bf9565848298",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a5cfe1a8-e3b1-4fd7-b806-2d58b2fd7858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Ollama endpoints\n",
    "ollama_base_url = \"http://127.0.0.1:11434\"\n",
    "ollama_chat = \"/api/chat\"\n",
    "ollama_embedding = \"/api/embeddings\"\n",
    "\n",
    "# Define Ollama models\n",
    "embedding_model = \"mxbai-embed-large\"\n",
    "llm_model = \"llama3.1:8b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa78c58-33c5-4365-8aeb-b6b07e1d1acd",
   "metadata": {},
   "source": [
    "## Key Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71428c7c-0648-4302-917e-1f60770d6ad8",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68fc158-e9d7-407f-b0b6-0b6981e03beb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "async def get_llm_response(in_text, timeout=60.0):\n",
    "    data = {\n",
    "        \"model\": llm_model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": in_text}\n",
    "        ],\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Create a client with an increased timeout\n",
    "        async with httpx.AsyncClient(timeout=httpx.Timeout(timeout)) as client:\n",
    "            response = await client.post((ollama_base_url + ollama_chat), json=data)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            \n",
    "            # Extract the response text based on Ollama's API structure\n",
    "            if \"message\" in result and \"content\" in result[\"message\"]:\n",
    "                return result[\"message\"][\"content\"].strip()\n",
    "            else:\n",
    "                print('Unexpected response structure:', result)\n",
    "                return None\n",
    "                \n",
    "    except httpx.ReadTimeout:\n",
    "        print(\"Request timed out. The Ollama server might be busy or the model is taking too long to respond.\")\n",
    "        return None\n",
    "    except httpx.ConnectError:\n",
    "        print(\"Could not connect to the Ollama server. Make sure it's running at http://127.0.0.1:11434.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a2305f4-2e2c-4a2e-927e-47481a9dd5e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "async def get_embeddings(text, model=, timeout=60.0):\n",
    "    \"\"\"\n",
    "    Get embeddings for the provided text using Ollama's API.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to generate embeddings for\n",
    "        model: The model to use for embeddings (default: \"llama3.1:8b\")\n",
    "        timeout: Timeout in seconds (default: 60 seconds)\n",
    "    \n",
    "    Returns:\n",
    "        A list of embedding values or None if an error occurs\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": text,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.0  # Lower temperature for more deterministic embeddings\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Create a client with an increased timeout\n",
    "        async with httpx.AsyncClient(timeout=httpx.Timeout(timeout)) as client:\n",
    "            response = await client.post((ollama_base_url + ollama_embedding), json=data)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            \n",
    "            # Extract the embeddings from the response\n",
    "            if \"embedding\" in result:\n",
    "                return result[\"embedding\"]\n",
    "            else:\n",
    "                print('No embeddings found in response:', result)\n",
    "                return None\n",
    "                \n",
    "    except httpx.ReadTimeout:\n",
    "        print(\"Request timed out. The Ollama server might be busy or the model is taking too long to respond.\")\n",
    "        return None\n",
    "    except httpx.ConnectError:\n",
    "        print(\"Could not connect to the Ollama server. Make sure it's running at http://127.0.0.1:11434.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec556fe-1011-48dc-9e48-5d9efdbefa1f",
   "metadata": {},
   "source": [
    "### Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5684e214-8da6-4a88-9db5-b5ed8b07fff3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "async def load_matrix():\n",
    "    return null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e9efb535-04e2-46f2-8b6a-1722f3c15afe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "async def append_matrix():\n",
    "    return null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c6fddc22-209e-4710-ab39-30845f05cba7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "async def find_nearest_k(embeddings, k):\n",
    "    return null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ac9249-9c7c-4486-9982-7061c1e2bb7c",
   "metadata": {},
   "source": [
    "### File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf596e0-e2aa-4d1b-8f21-4116079f5ec8",
   "metadata": {},
   "source": [
    "CSV and JSON will be critically important during the initial phases of development, though their continued presence is still up in the air. Overall I will come to prefer JSON for its robusticity, but CSV may come into play as I load in the initial conversation samples for processing. I intend for the final version of all of these testing objects to be JSON formatted\n",
    "\n",
    "As a side note: I recognize that these functions are entirely redundant, given this is python and read_csv is already native to the csv library, but defining these functions here is a mearsure to keep my thinking organized and clear, and any specific changes I will need to make to either csv or JSON can be made universally here\n",
    "                                                                                                                                                                                                                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6afefc40-4264-46f1-b5ac-79644ba8f590",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def read_csv(path):\n",
    "    csv_object = csv.read_csv(path)\n",
    "    return csv_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950647bb-1354-4c6b-a60f-e9b82adc2596",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def write_csv(data):\n",
    "    csv_object = csv.write_csv(data)\n",
    "    return csv_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1534be-71dc-4175-9d34-3abd60ac958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def read_json(path):\n",
    "    json_object = json.read_json(path)\n",
    "    return json_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c6e3c1-7583-4301-85ca-0c8e9c9b8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def write_json(data):\n",
    "    json_object = json.write_json(data)\n",
    "    return json_object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2dfd13-0837-40d1-ab6a-02ff9efe29e2",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56f1677-7bfa-4c8b-b76e-061d318bf8ee",
   "metadata": {},
   "source": [
    "### Basic\n",
    "\n",
    "Ensure that functions and apis are working as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d9134003-c5af-42f6-90a0-c2e4e01e403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"This is another test prompt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "13b80e94-8114-4edb-b7a3-54c7fb62b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await get_llm_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "46a37fc7-7eca-40e7-8601-0b84c64ad865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you're testing the system. I'm ready when you are. What would you like to talk about or do?\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1418731f-8787-416b-89de-5028eaf51fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = await get_embeddings(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3f98e653-d167-477d-90e5-ec3118d95769",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (3989306987.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[82], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(embeddings\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31m_IncompleteInputError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "print(embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060057a5-0e47-47d1-8cc8-45b8d25067e8",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Putting the pieces together and creating rudimentary workflows as proofs of concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e97e3d-28bd-49dd-baf4-26d8067b7983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f3819c-fe5b-4f0f-84a1-feff7833b245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
